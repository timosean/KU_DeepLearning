{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwnAWIF155ze",
        "outputId": "01ff6a37-2ab6-4a48-8f76-e18ae05e2b57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext==0.13.0\n",
            "  Downloading torchtext-0.13.0-cp38-cp38-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchtext==0.13.0) (1.21.6)\n",
            "Collecting torch==1.12.0\n",
            "  Downloading torch-1.12.0-cp38-cp38-manylinux1_x86_64.whl (776.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.3 MB 9.8 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torchtext==0.13.0) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchtext==0.13.0) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.12.0->torchtext==0.13.0) (4.4.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.13.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.13.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.13.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.13.0) (2022.12.7)\n",
            "Installing collected packages: torch, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.0+cu116\n",
            "    Uninstalling torch-1.13.0+cu116:\n",
            "      Successfully uninstalled torch-1.13.0+cu116\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.14.0\n",
            "    Uninstalling torchtext-0.14.0:\n",
            "      Successfully uninstalled torchtext-0.14.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.14.0+cu116 requires torch==1.13.0, but you have torch 1.12.0 which is incompatible.\n",
            "torchaudio 0.13.0+cu116 requires torch==1.13.0, but you have torch 1.12.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.12.0 torchtext-0.13.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchdata==0.4.0\n",
            "  Downloading torchdata-0.4.0-cp38-cp38-manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 6.2 MB/s \n",
            "\u001b[?25hCollecting portalocker>=2.0.0\n",
            "  Downloading portalocker-2.6.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchdata==0.4.0) (2.23.0)\n",
            "Requirement already satisfied: torch==1.12.0 in /usr/local/lib/python3.8/dist-packages (from torchdata==0.4.0) (1.12.0)\n",
            "Collecting urllib3>=1.25\n",
            "  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 69.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.12.0->torchdata==0.4.0) (4.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchdata==0.4.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchdata==0.4.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchdata==0.4.0) (2022.12.7)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 66.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: urllib3, portalocker, torchdata\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed portalocker-2.6.0 torchdata-0.4.0 urllib3-1.25.11\n"
          ]
        }
      ],
      "source": [
        "!pip install torchtext==0.13.0\n",
        "!pip install torchdata==0.4.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TBEtN5VF4YFi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import importlib\n",
        "\n",
        "from datetime import datetime as dt\n",
        "import time\n",
        "\n",
        "import imdb_voc\n",
        "\n",
        "\n",
        "root = './'\n",
        "\n",
        "# import sentences\n",
        "importlib.reload(imdb_voc)\n",
        "\n",
        "# set device\n",
        "dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsYRrXLr4i67",
        "outputId": "0512ed7b-3e2b-4900-a202-186e784ba8a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "negative\n",
            "i am oh soooo glad i have not spent money to go to the cinema on it - ) . it is nothing more than compilation of elements of few other classic titles like the thing , final fantasy , the abyss etc . framed in rather dull and meaningless scenario . i really can not figure out what was the purpose of creating this movie - it has absolutely nothing new to offer in its storyline which additionally is also senseless . moreover there is nothing to watch - the fx ' <UNK> look like there were taken from a second hand store , you generally saw all of them in other movies . but it is definitely a good <UNK> .\n",
            "\n",
            "positive\n",
            "of the three remakes on w . somerset <UNK> ' s novel , this one is the best one , and not particularly because what john cromwell brought to the film . the film is worth a look because of the break through performance by bette davis , who as mildred rogers , showed the film industry she was a star . finally , her struggles with jack warner and his studio paid off <UNK> . the film is dominated by mildred from the start . we realize from the beginning that mildred doesn ' t care for philip and never will . she doesn ' t hide her contempt for this kind soul that has fallen in love with the wrong woman . he will be humiliated by mildred again , and again , as she makes no bones about what she really is . poor philip carey , besides of being handicap , is a man who is weak . when he tries to cling onto mildred , she rejects him . it is when mildred returns to him , when she is frail and defeated , that he rises to the occasion , overcoming his own <UNK> on this terrible woman who has stolen his will and his manhood . bette davis gives a fantastic portrayal of mildred . this was one of her best roles and she ran away with it . her disgust toward the kind philip is clear from the onset of their relationship . when she tells him she <UNK> her mouth after he kisses her is one of the most powerful moment in the movie . leslie howard underplayed philip and makes him appear even weaker than he is . frances dee , reginald denny , alan hale and reginald owen , are seen in minor roles . this is bette davis show , and don ' t you forget it !\n",
            "\n",
            "positive\n",
            "this movie is one i strongly recommend . it ' s about a boy , stanley <UNK> ( shia <UNK> ) who is wrongly convicted of a crime and sent to camp green lake , a boys ' detention center . there , he is forced to dig holes 5 feet deep and 5 feet in <UNK> . while there , he meets the other boys of the camp ( zero , <UNK> , <UNK> , squid , <UNK> , and <UNK> ) . all of them are digging , not to ' build <UNK> ' , but to find outlaw kate <UNK> ' s treasure . throughout the movie ( and book ) stanley learns more about the past , more about himself , and more about digging holes . i give this movie a 9 . 5 , because , i am very picky when it comes to books to movies , ( i want the movie to follow the book exactly ) . but , still it did real well .\n",
            "\n",
            "negative\n",
            "i live in salt lake city and i ' m not a mormon , so why did i rent this movie ? well because i live in utah and thought it ' d be nice to see locations i know in a film . i really knew going into it that i wasn ' t going to get the inside jokes so i wasn ' t surprised when i sat with the deer in the headlights stare . what i was surprised at was the <UNK> mormon actions that were placed in this film . i know it ' s a mormon film , <UNK> to the members of the lds church , but i found it offensive because of the typical stereotype of people that isn ' t of their faith . every non mormon , which wasn ' t many , drank , smoked and had an amazing selfishness attitude , why ? that really ticked me off about this film , they made the mormons so pure , yet the rest of the state of utah i guess is filled with punk psychos just because they don ' t follow the <UNK> of the lds church . i can understand having the plots revolve around all lds members , but you ' d think salt lake city was 100% mormon , which isn ' t even close to being the truth . and as i said , the non mormons in the movie were portrayed as drunken jerks , please ! i guess i just don ' t get it because i don ' t belong to their faith and i guess i never will .\n",
            "\n",
            "\tAccuracy: 0.493\n",
            "Epoch: 01 | Time: 0m 8s\n",
            "\tTrain Loss: 0.693 | Val. Loss: 0.693\n",
            "\tAccuracy: 0.513\n",
            "Epoch: 02 | Time: 0m 8s\n",
            "\tTrain Loss: 0.693 | Val. Loss: 0.691\n",
            "\tAccuracy: 0.516\n",
            "Epoch: 03 | Time: 0m 8s\n",
            "\tTrain Loss: 0.692 | Val. Loss: 0.691\n",
            "\tAccuracy: 0.644\n",
            "Epoch: 04 | Time: 0m 8s\n",
            "\tTrain Loss: 0.679 | Val. Loss: 0.653\n",
            "\tAccuracy: 0.689\n",
            "Epoch: 05 | Time: 0m 8s\n",
            "\tTrain Loss: 0.631 | Val. Loss: 0.589\n",
            "\tAccuracy: 0.767\n",
            "Epoch: 06 | Time: 0m 8s\n",
            "\tTrain Loss: 0.562 | Val. Loss: 0.501\n",
            "\tAccuracy: 0.774\n",
            "Epoch: 07 | Time: 0m 8s\n",
            "\tTrain Loss: 0.496 | Val. Loss: 0.475\n",
            "\tAccuracy: 0.803\n",
            "Epoch: 08 | Time: 0m 8s\n",
            "\tTrain Loss: 0.487 | Val. Loss: 0.441\n",
            "\tAccuracy: 0.788\n",
            "Epoch: 09 | Time: 0m 8s\n",
            "\tTrain Loss: 0.457 | Val. Loss: 0.466\n",
            "\tAccuracy: 0.814\n",
            "Epoch: 10 | Time: 0m 8s\n",
            "\tTrain Loss: 0.446 | Val. Loss: 0.421\n",
            "\tAccuracy: 0.820\n",
            "Epoch: 11 | Time: 0m 8s\n",
            "\tTrain Loss: 0.401 | Val. Loss: 0.407\n",
            "\tAccuracy: 0.833\n",
            "Epoch: 12 | Time: 0m 8s\n",
            "\tTrain Loss: 0.407 | Val. Loss: 0.393\n",
            "\tAccuracy: 0.796\n",
            "Epoch: 13 | Time: 0m 8s\n",
            "\tTrain Loss: 0.411 | Val. Loss: 0.432\n",
            "\tAccuracy: 0.826\n",
            "Epoch: 14 | Time: 0m 8s\n",
            "\tTrain Loss: 0.361 | Val. Loss: 0.392\n",
            "\tAccuracy: 0.824\n",
            "Epoch: 15 | Time: 0m 8s\n",
            "\tTrain Loss: 0.343 | Val. Loss: 0.400\n",
            "\tAccuracy: 0.823\n",
            "Epoch: 16 | Time: 0m 8s\n",
            "\tTrain Loss: 0.346 | Val. Loss: 0.405\n",
            "\tAccuracy: 0.837\n",
            "Epoch: 17 | Time: 0m 8s\n",
            "\tTrain Loss: 0.324 | Val. Loss: 0.390\n",
            "\tAccuracy: 0.760\n",
            "Epoch: 18 | Time: 0m 8s\n",
            "\tTrain Loss: 0.310 | Val. Loss: 0.611\n",
            "\tAccuracy: 0.825\n",
            "Epoch: 19 | Time: 0m 8s\n",
            "\tTrain Loss: 0.357 | Val. Loss: 0.389\n",
            "\tAccuracy: 0.851\n",
            "Epoch: 20 | Time: 0m 8s\n",
            "\tTrain Loss: 0.298 | Val. Loss: 0.367\n",
            "\tAccuracy: 0.784\n",
            "Epoch: 21 | Time: 0m 8s\n",
            "\tTrain Loss: 0.291 | Val. Loss: 0.541\n",
            "\tAccuracy: 0.849\n",
            "Epoch: 22 | Time: 0m 8s\n",
            "\tTrain Loss: 0.286 | Val. Loss: 0.359\n",
            "\tAccuracy: 0.853\n",
            "Epoch: 23 | Time: 0m 8s\n",
            "\tTrain Loss: 0.282 | Val. Loss: 0.366\n",
            "\tAccuracy: 0.841\n",
            "Epoch: 24 | Time: 0m 8s\n",
            "\tTrain Loss: 0.271 | Val. Loss: 0.374\n",
            "\tAccuracy: 0.851\n",
            "Epoch: 25 | Time: 0m 8s\n",
            "\tTrain Loss: 0.266 | Val. Loss: 0.375\n",
            "\tAccuracy: 0.858\n",
            "Epoch: 26 | Time: 0m 8s\n",
            "\tTrain Loss: 0.268 | Val. Loss: 0.365\n",
            "\tAccuracy: 0.866\n",
            "Epoch: 27 | Time: 0m 8s\n",
            "\tTrain Loss: 0.252 | Val. Loss: 0.354\n",
            "\tAccuracy: 0.857\n",
            "Epoch: 28 | Time: 0m 8s\n",
            "\tTrain Loss: 0.239 | Val. Loss: 0.354\n",
            "\tAccuracy: 0.846\n",
            "Epoch: 29 | Time: 0m 8s\n",
            "\tTrain Loss: 0.241 | Val. Loss: 0.375\n",
            "\tAccuracy: 0.836\n",
            "Epoch: 30 | Time: 0m 8s\n",
            "\tTrain Loss: 0.220 | Val. Loss: 0.411\n",
            "*** Now test phase begins! ***\n",
            "\tAccuracy: 0.858\n",
            "| Test Loss: 0.360\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\"\"\"\n",
        "\n",
        "You can implement any necessary methods.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    '''\n",
        "    - d_model(int) : Transformer에서의 feature vector의 size\n",
        "    - d_Q, d_K, d_V(int) : size of Q, K, V for each head of the multi-head attention.\n",
        "                           Typically passed as (d_model / numhead) from TF_Encoder_Block\n",
        "    - numhead(int) : Multi-head attention에서 head의 개수\n",
        "    - dropout(float) : Dropout probaility\n",
        "    '''\n",
        "    def __init__(self, d_model, d_Q, d_K, d_V, numhead, dropout):    \n",
        "      super().__init__()\n",
        "      \n",
        "      self.numhead=numhead\n",
        "      # input linear layers for V, Q, K\n",
        "      # d_Q, d_K, d_V are typically set to d_model/numhead\n",
        "      \n",
        "      self.V_Linear = nn.Linear(in_features=d_model, out_features=d_model)\n",
        "      self.Q_Linear = nn.Linear(in_features=d_model, out_features=d_model)\n",
        "      self.K_Linear = nn.Linear(in_features=d_model, out_features=d_model)\n",
        "\n",
        "      # output linear layer\n",
        "      self.MHA_Linear = nn.Linear(in_features=d_model, out_features=d_model)\n",
        "      \n",
        "      # dropout\n",
        "      self.dropout=nn.Dropout(dropout)\n",
        "      \n",
        "    \n",
        "    def forward(self, x_Q, x_K, x_V, src_batch_lens=None):\n",
        "      # This method computes the scaled dot-product attention.\n",
        "      '''\n",
        "      1. x_Q, x_K, x_V(tensor) : Q, K, V inputs having shape (B, T_T, d_model), (B, T_S, d_model) and\n",
        "                                (B, T_S, d_model) respectively.\n",
        "        * B: batch size / T_S: source sequence length / T_T: target sequence length\n",
        "      2. src_batch_lens(tensor) : shape=(B, ), contains the length information of batched source.\n",
        "        만약 batch size가 3이고 input data(token of words from review)가 차례대로 길이가 3, 8, 5라면,\n",
        "        src_batch_lens는 [3,8,5]가 된다.\n",
        "        만약, T_S=10이라고 치면, batch의 first input의 길이가 3이었으므로 3 word token과 7 <PAD> token을 가진다.\n",
        "        Note these <PAD> tokens should be ignored when we compute the attention coefficients. That is, the attention\n",
        "        coefficient computed from softmax operation should be sufficient small on input positions with <PAD>.\n",
        "        ***Use src_batch_lens to find out which part of source input is <PAD>!***\n",
        "      '''\n",
        "      # Q2. Implement\n",
        "      # out: tensor, shape=(B, T_T, d_model)\n",
        "      # Operation\n",
        "      '''\n",
        "      - Inputs x_Q, x_K, x_V is first projected to each head through linear layers (Fig 2)\n",
        "      Then the following Scaled Dot-Product Attention is Fig 2 is applied \"to each head\".\n",
        "      (uses Matmul, Scale, Softmax)\n",
        "      - Mask(opt.) layer masks out source tokens which are <PAD>, so that they have negligible effect on computing\n",
        "      softmax. The masking can be achieved using \"src_batch_lens\"\n",
        "      - Where to put dropout?\n",
        "        (1) Before applying attention coefficients to V. (softmax까지 계산하고 V랑 곱하기 전에)\n",
        "        (2) After applying the final Linear layer => 즉, right before returning 'out', apply dropout.\n",
        "      '''\n",
        "      d_k = x_K.shape[-1]\n",
        "      \n",
        "      attention_score = torch.matmul(x_Q, x_K.transpose(-2, -1))\n",
        "      attention_score = attention_score / math.sqrt(d_k)\n",
        "\n",
        "      # print(attention_score.shape) # (256, 384, 384)\n",
        "\n",
        "      # mask 적용하기\n",
        "      res = attention_score.clone()\n",
        "      res[attention_score=='<PAD>'] = 1e-21\n",
        "      attention_score = res\n",
        "\n",
        "\n",
        "      # softmax 적용하기\n",
        "      attention_prob = F.softmax(attention_score, dim=-1)\n",
        "      attention_prob = self.dropout(attention_prob)\n",
        "\n",
        "      # V와 최종적으로 matmul\n",
        "      out = torch.matmul(attention_prob, x_V)\n",
        "      out = self.dropout(out)\n",
        "\n",
        "      return out\n",
        "\n",
        "class TF_Encoder_Block(nn.Module):\n",
        "    '''\n",
        "    - d_model(int) : Transformer에서의 feature vector의 size\n",
        "    - d_ff(int) : Feed Forward block의 feature vector의 size\n",
        "    - numhead(int) : Multi-head attention에서 head의 개수\n",
        "    - dropout(float) : Dropout probaility\n",
        "    '''\n",
        "    def __init__(self, d_model, d_ff, numhead, dropout):\n",
        "      # Q3. Implment constructor for transformer encoder block    \n",
        "      super().__init__()\n",
        "      \n",
        "      self.self_attn_layer_norm = nn.LayerNorm(d_model)\n",
        "      self.ff_layer_norm = nn.LayerNorm(d_model)\n",
        "      self.self_attention = MultiHeadAttention(d_model, d_model/numhead, d_model/numhead, d_model/numhead, numhead, dropout)\n",
        "      \n",
        "      self.Linear1 = nn.Linear(in_features=d_model, out_features=d_ff)\n",
        "      self.Linear2 = nn.Linear(in_features=d_ff, out_features=d_model)\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "      self.feedforward_block = nn.Sequential(self.Linear1, nn.ReLU(), self.dropout, self.Linear2, self.dropout)\n",
        "      \n",
        "\n",
        "    def forward(self, x, src_batch_lens):\n",
        "        '''\n",
        "        - d model :int, size of feature vector in the Transformer.\n",
        "        - x :tensor, x input feature having shape (B, T_S, d model).\n",
        "        - src batch lens :Same as explained previously. You should pass src batch lens\n",
        "                          to your MultiHeadAttention object instantiated in this class.\n",
        "        '''\n",
        "      \n",
        "        # Q4. Implment forward function for transformer encoder block\n",
        "        '''\n",
        "        [Operation]\n",
        "        forward function should perform:\n",
        "        1. Feed input x to multi-head attention layer. Note that the operation of this layer\n",
        "           is self-attention, so set the input properly.\n",
        "        2. attention output is added to x (skip connection), then perform layer normalization\n",
        "        3. then the output is fed into feed forward layer\n",
        "        4. feed forward output is added to its input (skip connection), then perform layer\n",
        "           normalization\n",
        "        '''\n",
        "\n",
        "        mha_out = self.self_attention.forward(x, x, x, src_batch_lens)\n",
        "        layernorm_out1 = self.self_attn_layer_norm(x + mha_out)\n",
        "        ff_out = self.feedforward_block(layernorm_out1)\n",
        "        out = self.ff_layer_norm(layernorm_out1 + ff_out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\"\"\"\n",
        "Positional encoding\n",
        "PE(pos,2i) = sin(pos/10000**(2i/dmodel))\n",
        "PE(pos,2i+1) = cos(pos/10000**(2i/dmodel))\n",
        "\"\"\"\n",
        "\n",
        "def PosEncoding(t_len, d_model):\n",
        "    i = torch.tensor(range(d_model))\n",
        "    pos = torch.tensor(range(t_len))\n",
        "    POS, I = torch.meshgrid(pos, i)\n",
        "    PE = (1-I % 2)*torch.sin(POS/10**(4*I/d_model)) + (I%2)*torch.cos(POS/10**(4*(I-1)/d_model))\n",
        "    return PE\n",
        "\n",
        "class TF_Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model,\n",
        "                 d_ff, numlayer, numhead, dropout):    \n",
        "        super().__init__()\n",
        "        \n",
        "        self.numlayer = numlayer\n",
        "        self.src_embed  = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "        '''\n",
        "        - vocab size, :int, size of vocabulary, i.e., the total number of words recognized\n",
        "                       by the model.\n",
        "        - d model :int, size of feature vector in the Transformer.\n",
        "        - d ff :int, size of feature vector in Feed Forward block.\n",
        "        - numlayer :int, number of TF Encoder Block in the encoder (N in Fig. 1)\n",
        "        - numhead :int, number of heads in multi-head attention.\n",
        "        - dropout :float, dropout probability.\n",
        "        '''\n",
        "\n",
        "        # Q5. Implement a sequence of numlayer encoder blocks\n",
        "        self.layers = nn.ModuleList([TF_Encoder_Block(d_model, d_ff, numhead, dropout) for _ in range(numlayer)])\n",
        "        \n",
        "    def forward(self, x, src_batch_lens):\n",
        "\n",
        "      x_embed = self.src_embed(x)\n",
        "      x = self.dropout(x_embed)\n",
        "      p_enc = PosEncoding(x.shape[1], x.shape[2]).to(dev)\n",
        "      x = x + p_enc\n",
        "        \n",
        "      # Q6. Implement: forward over numlayer encoder blocks\n",
        "      '''\n",
        "      - x :tensor, a batch of input tokens having shape (B,T_S). B is batch size, T_S is\n",
        "           the sequence length of tokens. Regardless of the length of review words in each\n",
        "           batch, each batch is padded to length T_S.\n",
        "      - src batch lens :Same as explained previously. You should pass src batch lens\n",
        "                        to your MultiHeadAttention object instantiated in this class.\n",
        "      '''\n",
        "      for layer in self.layers:\n",
        "        x = layer(x, src_batch_lens)\n",
        "      \n",
        "      return x\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "main model\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class sentiment_classifier(nn.Module):\n",
        "    \n",
        "    def __init__(self, enc_input_size, \n",
        "                 enc_d_model,\n",
        "                 enc_d_ff,\n",
        "                 enc_num_layer,\n",
        "                 enc_num_head,\n",
        "                 dropout,\n",
        "                ):    \n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = TF_Encoder(vocab_size = enc_input_size,\n",
        "                                  d_model = enc_d_model, d_ff=enc_d_ff,\n",
        "                                  numlayer=enc_num_layer, numhead=enc_num_head,\n",
        "                                  dropout=dropout)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1,None)),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(in_features = enc_d_model, out_features=enc_d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(in_features = enc_d_model, out_features = 1)\n",
        "        )\n",
        "          \n",
        "   \n",
        "    def forward(self, x, x_lens):\n",
        "        src_ctx = self.encoder(x, src_batch_lens = x_lens)\n",
        "        # size should be (b,)\n",
        "        out_logits = self.classifier(src_ctx).flatten()\n",
        "\n",
        "        return out_logits\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "datasets\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Load IMDB dataset\n",
        "# once you build the dataset, you can load it from file to save time\n",
        "# to load from file, set this flag True\n",
        "load_imdb_dataset = True\n",
        "\n",
        "if load_imdb_dataset:\n",
        "    imdb_dataset = torch.load('imdb_dataset.pt')\n",
        "else:\n",
        "    imdb_dataset = imdb_voc.IMDB_tensor_dataset()\n",
        "    torch.save(imdb_dataset, 'imdb_dataset.pt')\n",
        "\n",
        "train_dataset, test_dataset = imdb_dataset.get_dataset()\n",
        "\n",
        "split_ratio = 0.85\n",
        "num_train = int(len(train_dataset) * split_ratio)\n",
        "split_train, split_valid = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
        "\n",
        "# Set hyperparam (batch size)\n",
        "batch_size_trn = 256\n",
        "batch_size_val = 256\n",
        "batch_size_tst = 256\n",
        "\n",
        "train_dataloader = DataLoader(split_train, batch_size=batch_size_trn, shuffle=True)\n",
        "val_dataloader = DataLoader(split_valid, batch_size=batch_size_val, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size_tst, shuffle=True)\n",
        "\n",
        "# get character dictionary\n",
        "src_word_dict = imdb_dataset.src_stoi\n",
        "src_idx_dict = imdb_dataset.src_itos\n",
        "\n",
        "SRC_PAD_IDX = src_word_dict['<PAD>']\n",
        "\n",
        "# show sample reviews with pos/neg sentiments\n",
        "\n",
        "show_sample_reviews = True\n",
        "\n",
        "if show_sample_reviews:\n",
        "    \n",
        "    sample_text, sample_lab = next(iter(train_dataloader))\n",
        "    slist=[]\n",
        "\n",
        "    for stxt in sample_text[:4]: \n",
        "        slist.append([src_idx_dict[j] for j in stxt])\n",
        "\n",
        "    for j, s in enumerate(slist):\n",
        "        print('positive' if sample_lab[j]==1 else 'negative')\n",
        "        print(' '.join([i for i in s if i != '<PAD>'])+'\\n')\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "model\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "enc_vocab_size = len(src_word_dict) # counting eof, one-hot vector goes in\n",
        "\n",
        "# Set hyperparam (model size)\n",
        "# examples: model & ff dim - 8, 16, 32, 64, 128, numhead, numlayer 1~4\n",
        "\n",
        "enc_d_model = 16\n",
        "\n",
        "enc_d_ff = 16\n",
        "\n",
        "enc_num_head = 4\n",
        "\n",
        "enc_num_layer= 4\n",
        "\n",
        "DROPOUT=0.1\n",
        "\n",
        "model = sentiment_classifier(enc_input_size=enc_vocab_size,\n",
        "                         enc_d_model = enc_d_model,     \n",
        "                         enc_d_ff = enc_d_ff, \n",
        "                         enc_num_head = enc_num_head, \n",
        "                         enc_num_layer = enc_num_layer,\n",
        "                         dropout=DROPOUT) \n",
        "\n",
        "model = model.to(dev)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "optimizer\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Set hyperparam (learning rate)\n",
        "# examples: 1e-3 ~ 1e-5\n",
        "\n",
        "lr = 1e-3\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "auxiliary functions\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# get length of reviews in batch\n",
        "def get_lens_from_tensor(x):\n",
        "    # lens (batch, t)\n",
        "    lens = torch.ones_like(x).long()\n",
        "    lens[x==SRC_PAD_IDX]=0\n",
        "    return torch.sum(lens, dim=-1)\n",
        "\n",
        "def get_binary_metrics(y_pred, y):\n",
        "    # find number of TP, TN, FP, FN\n",
        "    TP=sum(((y_pred == 1)&(y==1)).type(torch.int32))\n",
        "    FP=sum(((y_pred == 1)&(y==0)).type(torch.int32))\n",
        "    TN=sum(((y_pred == 0)&(y==0)).type(torch.int32))\n",
        "    FN=sum(((y_pred == 0)&(y==1)).type(torch.int32))\n",
        "    accy = (TP+TN)/(TP+FP+TN+FN)\n",
        "            \n",
        "    recall = TP/(TP+FN) if TP+FN!=0 else 0\n",
        "    prec = TP/(TP+FP) if TP+FP!=0 else 0\n",
        "    f1 = 2*recall*prec/(recall+prec) if recall+prec !=0 else 0\n",
        "    \n",
        "    return accy, recall, prec, f1\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "train/validation\n",
        "\n",
        "\"\"\" \n",
        "\n",
        "\n",
        "def train(model, dataloader, optimizer, criterion, clip):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i, batch in enumerate(dataloader):\n",
        "\n",
        "        src = batch[0].to(dev)\n",
        "        trg = batch[1].float().to(dev)\n",
        "\n",
        "        # print('batch trg.shape', trg.shape)\n",
        "        # print('batch src.shape', src.shape)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x_lens = get_lens_from_tensor(src).to(dev)\n",
        "\n",
        "        output = model(x=src, x_lens=x_lens) \n",
        "\n",
        "\n",
        "        output = output.contiguous().view(-1)\n",
        "        trg = trg.contiguous().view(-1)\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader)\n",
        "\n",
        "def evaluate(model, dataloader, criterion):\n",
        "\n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    epoch_accy =0\n",
        "    epoch_recall =0\n",
        "    epoch_prec =0\n",
        "    epoch_f1 =0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(dataloader):\n",
        "\n",
        "            src = batch[0].to(dev)\n",
        "            trg = batch[1].float().to(dev)\n",
        "\n",
        "            x_lens = get_lens_from_tensor(src).to(dev)\n",
        "\n",
        "            output = model(x=src, x_lens=x_lens) \n",
        "\n",
        "            output = output.contiguous().view(-1)\n",
        "            trg = trg.contiguous().view(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            \n",
        "            accy, recall, prec, f1 = get_binary_metrics((output>=0).long(), trg.long())\n",
        "            epoch_accy += accy\n",
        "            epoch_recall += recall\n",
        "            epoch_prec += prec\n",
        "            epoch_f1 += f1\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    # show accuracy\n",
        "    print(f'\\tAccuracy: {epoch_accy/(len(dataloader)):.3f}')\n",
        "    \n",
        "    return epoch_loss / len(dataloader)\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Training loop\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "N_EPOCHS = 30\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train(model, train_dataloader, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, val_dataloader, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'model.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f}')\n",
        "        \n",
        "\"\"\"\n",
        "\n",
        "Test loop\n",
        "\n",
        "\"\"\"\n",
        "print('*** Now test phase begins! ***')\n",
        "model.load_state_dict(torch.load('model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_dataloader, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "24cbd48528ef30644a4c0b3b48f30a048f2ce882986e8eaa6af7187cf578eebd"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
