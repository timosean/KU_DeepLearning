{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\이승우학부휴학컴퓨터학과\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatetime\u001b[39;00m \u001b[39mimport\u001b[39;00m datetime \u001b[39mas\u001b[39;00m dt\n\u001b[0;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtime\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mimdb_voc\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\이승우학부휴학컴퓨터학과\\Desktop\\고려대학교\\3학년\\2학기\\딥러닝\\프로그래밍 과제\\imdb_voc.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrnn\u001b[39;00m \u001b[39mimport\u001b[39;00m pad_sequence\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataset\u001b[39;00m \u001b[39mimport\u001b[39;00m random_split\n\u001b[1;32m----> 6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m\u001b[39m,\u001b[39m \u001b[39mtorchdata\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m IMDB\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m get_tokenizer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import importlib\n",
    "\n",
    "from datetime import datetime as dt\n",
    "import time\n",
    "\n",
    "import imdb_voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "\n",
    "You can implement any necessary methods.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    '''\n",
    "    - d_model(int) : Transformer에서의 feature vector의 size\n",
    "    - d_Q, d_K, d_V(int) : size of Q, K, V for each head of the multi-head attention.\n",
    "                           Typically passed as (d_model / numhead) from TF_Encoder_Block\n",
    "    - numhead(int) : Multi-head attention에서 head의 개수\n",
    "    - dropout(float) : Dropout probaility\n",
    "    '''\n",
    "    def __init__(self, d_model, d_Q, d_K, d_V, numhead, dropout):    \n",
    "      super().__init__()\n",
    "      \n",
    "      self.numhead=numhead\n",
    "      # input linear layers for V, Q, K\n",
    "      # d_Q, d_K, d_V are typically set to d_model/numhead\n",
    "      \n",
    "      self.V_Linear = nn.Linear(in_features=d_model, out_features=d_V*numhead)\n",
    "      self.Q_Linear = nn.Linear(in_features=d_model, out_features=d_K*numhead)\n",
    "      self.K_Linear = nn.Linear(in_features=d_model, out_features=d_Q*numhead)\n",
    "\n",
    "      # output linear layer\n",
    "      self.MHA_Linear = nn.Linear(in_features=d_V*numhead, out_features=d_model)\n",
    "      \n",
    "      # dropout\n",
    "      self.dropout=nn.Dropout(dropout)\n",
    "      \n",
    "    \n",
    "    def forward(self, x_Q, x_K, x_V, src_batch_lens=None):\n",
    "      # This method computes the scaled dot-product attention.\n",
    "      '''\n",
    "      1. x_Q, x_K, x_V(tensor) : Q, K, V inputs having shape (B, T_T, d_model), (B, T_S, d_model) and\n",
    "                                (B, T_S, d_model) respectively.\n",
    "        * B: batch size / T_S: source sequence length / T_T: target sequence length\n",
    "      2. src_batch_lens(tensor) : shape=(B, ), contains the length information of batched source.\n",
    "        만약 batch size가 3이고 input data(token of words from review)가 차례대로 길이가 3, 8, 5라면,\n",
    "        src_batch_lens는 [3,8,5]가 된다.\n",
    "        만약, T_S=10이라고 치면, batch의 first input의 길이가 3이었으므로 3 word token과 7 <PAD> token을 가진다.\n",
    "        Note these <PAD> tokens should be ignored when we compute the attention coefficients. That is, the attention\n",
    "        coefficient computed from softmax operation should be sufficient small on input positions with <PAD>.\n",
    "        ***Use src_batch_lens to find out which part of source input is <PAD>!***\n",
    "      '''\n",
    "      # Q2. Implement\n",
    "      # out: tensor, shape=(B, T_T, d_model)\n",
    "      # Operation\n",
    "      '''\n",
    "      - Inputs x_Q, x_K, x_V is first projected to each head through linear layers (Fig 2)\n",
    "      Then the following Scaled Dot-Product Attention is Fig 2 is applied \"to each head\".\n",
    "      (uses Matmul, Scale, Softmax)\n",
    "      - Mask(opt.) layer masks out source tokens which are <PAD>, so that they have negligible effect on computing\n",
    "      softmax. The masking can be achieved using \"src_batch_lens\"\n",
    "      - Where to put dropout?\n",
    "        (1) Before applying attention coefficients to V. (softmax까지 계산하고 V랑 곱하기 전에)\n",
    "        (2) After applying the final Linear layer => 즉, right before returning 'out', apply dropout.\n",
    "      '''\n",
    "      d_k = x_K.shape[-1]\n",
    "      \n",
    "      attention_score = torch.matmul(x_Q, x_K.transpose(-2, -1))\n",
    "      attention_score = attention_score / math.sqrt(d_k)\n",
    "\n",
    "      # mask 적용하기\n",
    "      for i in range(src_batch_lens.shape[0]):\n",
    "        start_idx = src_batch_lens[i]\n",
    "        score_len = attention_score.shape[0]\n",
    "        for j in range(score_len):\n",
    "          attention_score[j] = 0\n",
    "          j = j + 1\n",
    "\n",
    "      # softmax 적용하기\n",
    "      attention_prob = F.softmax(attention_score, dim=-1)\n",
    "      attention_prob = self.dropout(attention_prob)\n",
    "\n",
    "      # V와 최종적으로 matmul\n",
    "      out = torch.matmul(attention_prob, x_V)\n",
    "      out = self.dropout(out)\n",
    "\n",
    "      return out\n",
    "\n",
    "class TF_Encoder_Block(nn.Module):\n",
    "    '''\n",
    "    - d_model(int) : Transformer에서의 feature vector의 size\n",
    "    - d_ff(int) : Feed Forward block의 feature vector의 size\n",
    "    - numhead(int) : Multi-head attention에서 head의 개수\n",
    "    - dropout(float) : Dropout probaility\n",
    "    '''\n",
    "    def __init__(self, d_model, d_ff, numhead, dropout):\n",
    "      # Q3. Implment constructor for transformer encoder block    \n",
    "      super().__init__()\n",
    "      \n",
    "      self.self_attn_layer_norm = nn.LayerNorm(d_model)\n",
    "      self.ff_layer_norm = nn.LayerNorm(d_model)\n",
    "      self.self_attention = MultiHeadAttention(d_model, d_model/numhead, d_model/numhead, d_model/numhead, numhead, dropout)\n",
    "      \n",
    "      self.Linear1 = nn.Linear(in_features=d_model, out_features=d_ff)\n",
    "      self.Linear2 = nn.Linear(in_features=d_ff, out_features=d_model)\n",
    "      self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "      self.feedforward_block = nn.Sequential(self.Linear1, nn.ReLU(), self.Dropout, self.Linear2, self.Dropout)\n",
    "      \n",
    "\n",
    "    def forward(self, x, src_batch_lens):\n",
    "        '''\n",
    "        - d model :int, size of feature vector in the Transformer.\n",
    "        - x :tensor, x input feature having shape (B, T_S, d model).\n",
    "        - src batch lens :Same as explained previously. You should pass src batch lens\n",
    "                          to your MultiHeadAttention object instantiated in this class.\n",
    "        '''\n",
    "      \n",
    "        # Q4. Implment forward function for transformer encoder block\n",
    "        '''\n",
    "        [Operation]\n",
    "        forward function should perform:\n",
    "        1. Feed input x to multi-head attention layer. Note that the operation of this layer\n",
    "           is self-attention, so set the input properly.\n",
    "        2. attention output is added to x (skip connection), then perform layer normalization\n",
    "        3. then the output is fed into feed forward layer\n",
    "        4. feed forward output is added to its input (skip connection), then perform layer\n",
    "           normalization\n",
    "        '''\n",
    "\n",
    "        mha_out = self.self_attention.forward(x, x, x, src_batch_lens)\n",
    "        layernorm_out1 = self.self_attn_layer_norm(x + mha_out)\n",
    "        ff_out = self.feedforward_block(layernorm_out1)\n",
    "        out = self.ff_layer_norm(layernorm_out1 + ff_out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\"\"\"\n",
    "Positional encoding\n",
    "PE(pos,2i) = sin(pos/10000**(2i/dmodel))\n",
    "PE(pos,2i+1) = cos(pos/10000**(2i/dmodel))\n",
    "\"\"\"\n",
    "\n",
    "def PosEncoding(t_len, d_model):\n",
    "    i = torch.tensor(range(d_model))\n",
    "    pos = torch.tensor(range(t_len))\n",
    "    POS, I = torch.meshgrid(pos, i)\n",
    "    PE = (1-I % 2)*torch.sin(POS/10**(4*I/d_model)) + (I%2)*torch.cos(POS/10**(4*(I-1)/d_model))\n",
    "    return PE\n",
    "\n",
    "class TF_Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model,\n",
    "                 d_ff, numlayer, numhead, dropout):    \n",
    "        super().__init__()\n",
    "        \n",
    "        self.numlayer = numlayer\n",
    "        self.src_embed  = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "        '''\n",
    "        - vocab size, :int, size of vocabulary, i.e., the total number of words recognized\n",
    "                       by the model.\n",
    "        - d model :int, size of feature vector in the Transformer.\n",
    "        - d ff :int, size of feature vector in Feed Forward block.\n",
    "        - numlayer :int, number of TF Encoder Block in the encoder (N in Fig. 1)\n",
    "        - numhead :int, number of heads in multi-head attention.\n",
    "        - dropout :float, dropout probability.\n",
    "        '''\n",
    "\n",
    "        # Q5. Implement a sequence of numlayer encoder blocks\n",
    "        self.layers = nn.ModuleList([TF_Encoder_Block(d_model, d_ff, numhead, dropout) for _ in range(numlayer)])\n",
    "        \n",
    "    def forward(self, x, src_batch_lens):\n",
    "\n",
    "      x_embed = self.src_embed(x)\n",
    "      x = self.dropout(x_embed)\n",
    "      p_enc = PosEncoding(x.shape[1], x.shape[2]).to(dev)\n",
    "      x = x + p_enc\n",
    "        \n",
    "      # Q6. Implement: forward over numlayer encoder blocks\n",
    "      '''\n",
    "      - x :tensor, a batch of input tokens having shape (B,T_S). B is batch size, T_S is\n",
    "           the sequence length of tokens. Regardless of the length of review words in each\n",
    "           batch, each batch is padded to length T_S.\n",
    "      - src batch lens :Same as explained previously. You should pass src batch lens\n",
    "                        to your MultiHeadAttention object instantiated in this class.\n",
    "      '''\n",
    "      for layer in self.layers:\n",
    "        x = layer(x)\n",
    "      \n",
    "      return x\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "main model\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class sentiment_classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, enc_input_size, \n",
    "                 enc_d_model,\n",
    "                 enc_d_ff,\n",
    "                 enc_num_layer,\n",
    "                 enc_num_head,\n",
    "                 dropout,\n",
    "                ):    \n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = TF_Encoder(vocab_size = enc_input_size,\n",
    "                                  d_model = enc_d_model, d_ff=enc_d_ff,\n",
    "                                  numlayer=enc_num_layer, numhead=enc_num_head,\n",
    "                                  dropout=dropout)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1,None)),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_features = enc_d_model, out_features=enc_d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_features = enc_d_model, out_features = 1)\n",
    "        )\n",
    "          \n",
    "   \n",
    "    def forward(self, x, x_lens):\n",
    "        src_ctx = self.encoder(x, src_batch_lens = x_lens)\n",
    "        # size should be (b,)\n",
    "        out_logits = self.classifier(src_ctx).flatten()\n",
    "\n",
    "        return out_logits\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "datasets\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Load IMDB dataset\n",
    "# once you build the dataset, you can load it from file to save time\n",
    "# to load from file, set this flag True\n",
    "load_imdb_dataset = True\n",
    "\n",
    "if load_imdb_dataset:\n",
    "    imdb_dataset = torch.load('imdb_dataset.pt')\n",
    "else:\n",
    "    imdb_dataset = imdb_voc.IMDB_tensor_dataset()\n",
    "    torch.save(imdb_dataset, 'imdb_dataset.pt')\n",
    "\n",
    "train_dataset, test_dataset = imdb_dataset.get_dataset()\n",
    "\n",
    "split_ratio = 0.85\n",
    "num_train = int(len(train_dataset) * split_ratio)\n",
    "split_train, split_valid = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "# Set hyperparam (batch size)\n",
    "batch_size_trn = 256\n",
    "batch_size_val = 256\n",
    "batch_size_tst = 256\n",
    "\n",
    "train_dataloader = DataLoader(split_train, batch_size=batch_size_trn, shuffle=True)\n",
    "val_dataloader = DataLoader(split_valid, batch_size=batch_size_val, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size_tst, shuffle=True)\n",
    "\n",
    "# get character dictionary\n",
    "src_word_dict = imdb_dataset.src_stoi\n",
    "src_idx_dict = imdb_dataset.src_itos\n",
    "\n",
    "SRC_PAD_IDX = src_word_dict['<PAD>']\n",
    "\n",
    "# show sample reviews with pos/neg sentiments\n",
    "\n",
    "show_sample_reviews = True\n",
    "\n",
    "if show_sample_reviews:\n",
    "    \n",
    "    sample_text, sample_lab = next(iter(train_dataloader))\n",
    "    slist=[]\n",
    "\n",
    "    for stxt in sample_text[:4]: \n",
    "        slist.append([src_idx_dict[j] for j in stxt])\n",
    "\n",
    "    for j, s in enumerate(slist):\n",
    "        print('positive' if sample_lab[j]==1 else 'negative')\n",
    "        print(' '.join([i for i in s if i != '<PAD>'])+'\\n')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "model\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "enc_vocab_size = len(src_word_dict) # counting eof, one-hot vector goes in\n",
    "\n",
    "# Set hyperparam (model size)\n",
    "# examples: model & ff dim - 8, 16, 32, 64, 128, numhead, numlayer 1~4\n",
    "\n",
    "enc_d_model = 8\n",
    "\n",
    "enc_d_ff = 8\n",
    "\n",
    "enc_num_head = 2\n",
    "\n",
    "enc_num_layer= 2\n",
    "\n",
    "DROPOUT=0.1\n",
    "\n",
    "model = sentiment_classifier(enc_input_size=enc_vocab_size,\n",
    "                         enc_d_model = enc_d_model,     \n",
    "                         enc_d_ff = enc_d_ff, \n",
    "                         enc_num_head = enc_num_head, \n",
    "                         enc_num_layer = enc_num_layer,\n",
    "                         dropout=DROPOUT) \n",
    "\n",
    "model = model.to(dev)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "optimizer\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Set hyperparam (learning rate)\n",
    "# examples: 1e-3 ~ 1e-5\n",
    "\n",
    "lr = 1e-4\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "auxiliary functions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# get length of reviews in batch\n",
    "def get_lens_from_tensor(x):\n",
    "    # lens (batch, t)\n",
    "    lens = torch.ones_like(x).long()\n",
    "    lens[x==SRC_PAD_IDX]=0\n",
    "    return torch.sum(lens, dim=-1)\n",
    "\n",
    "def get_binary_metrics(y_pred, y):\n",
    "    # find number of TP, TN, FP, FN\n",
    "    TP=sum(((y_pred == 1)&(y==1)).type(torch.int32))\n",
    "    FP=sum(((y_pred == 1)&(y==0)).type(torch.int32))\n",
    "    TN=sum(((y_pred == 0)&(y==0)).type(torch.int32))\n",
    "    FN=sum(((y_pred == 0)&(y==1)).type(torch.int32))\n",
    "    accy = (TP+TN)/(TP+FP+TN+FN)\n",
    "            \n",
    "    recall = TP/(TP+FN) if TP+FN!=0 else 0\n",
    "    prec = TP/(TP+FP) if TP+FP!=0 else 0\n",
    "    f1 = 2*recall*prec/(recall+prec) if recall+prec !=0 else 0\n",
    "    \n",
    "    return accy, recall, prec, f1\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "train/validation\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion, clip):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(dataloader):\n",
    "\n",
    "        src = batch[0].to(dev)\n",
    "        trg = batch[1].float().to(dev)\n",
    "\n",
    "        # print('batch trg.shape', trg.shape)\n",
    "        # print('batch src.shape', src.shape)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x_lens = get_lens_from_tensor(src).to(dev)\n",
    "\n",
    "        output = model(x=src, x_lens=x_lens) \n",
    "\n",
    "\n",
    "        output = output.contiguous().view(-1)\n",
    "        trg = trg.contiguous().view(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    epoch_accy =0\n",
    "    epoch_recall =0\n",
    "    epoch_prec =0\n",
    "    epoch_f1 =0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "\n",
    "            src = batch[0].to(dev)\n",
    "            trg = batch[1].float().to(dev)\n",
    "\n",
    "            x_lens = get_lens_from_tensor(src).to(dev)\n",
    "\n",
    "            output = model(x=src, x_lens=x_lens) \n",
    "\n",
    "            output = output.contiguous().view(-1)\n",
    "            trg = trg.contiguous().view(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            accy, recall, prec, f1 = get_binary_metrics((output>=0).long(), trg.long())\n",
    "            epoch_accy += accy\n",
    "            epoch_recall += recall\n",
    "            epoch_prec += prec\n",
    "            epoch_f1 += f1\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    # show accuracy\n",
    "    print(f'\\tAccuracy: {epoch_accy/(len(dataloader)):.3f}')\n",
    "    \n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Training loop\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_dataloader, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, val_dataloader, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f}')\n",
    "        \n",
    "\"\"\"\n",
    "\n",
    "Test loop\n",
    "\n",
    "\"\"\"\n",
    "print('*** Now test phase begins! ***')\n",
    "model.load_state_dict(torch.load('model.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_dataloader, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24cbd48528ef30644a4c0b3b48f30a048f2ce882986e8eaa6af7187cf578eebd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
