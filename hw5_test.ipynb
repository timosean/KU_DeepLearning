{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'nn_layers_pt' from 'c:\\\\Users\\\\이승우학부휴학컴퓨터학과\\\\Desktop\\\\고려대학교\\\\3학년\\\\2학기\\\\딥러닝\\\\프로그래밍 과제\\\\nn_layers_pt.py'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "HW5\n",
    "\n",
    "\"\"\"\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import importlib\n",
    "import pickle\n",
    "import numbers\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "import nn_layers_pt as nnl\n",
    "importlib.reload(nnl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn_mnist_classifier:\n",
    "    def __init__(self, mmt_friction=0.9, lr=1e-2):\n",
    "        ## initialize each layer of the entire classifier\n",
    "\n",
    "        # convolutional layer\n",
    "        # input image size: 28 x 28\n",
    "        # filter size: 3 x 3\n",
    "        # input channel size: 1\n",
    "        # output channel size (number of filters): 28\n",
    "\n",
    "        self.conv_layer_1 = nnl.nn_convolutional_layer(f_height=3, f_width=3, input_size=28,\n",
    "                                                       in_ch_size=1, out_ch_size=28)\n",
    "\n",
    "        # activation: relu\n",
    "        self.act_1 = nnl.nn_activation_layer()\n",
    "\n",
    "        # activaition map output: map size 26 x 26, 28 channels\n",
    "\n",
    "        # maxpool\n",
    "        self.maxpool_layer_1 = nnl.nn_max_pooling_layer(pool_size=2, stride=2)\n",
    "\n",
    "        # after max pool, map size 13 x 13, 28 channels\n",
    "\n",
    "        # fully connected layer 1\n",
    "        # input: 13 x 13 with 28 channels\n",
    "        # output 128\n",
    "        self.fc1 = nnl.nn_fc_layer(input_size=28*13*13, output_size=128)\n",
    "        self.act_2 = nnl.nn_activation_layer()\n",
    "\n",
    "        # fully connected layer 1\n",
    "        # input 128\n",
    "        # output 10\n",
    "        self.fc2 = nnl.nn_fc_layer(input_size=128, output_size=10)\n",
    "\n",
    "        # softmax\n",
    "        self.sm1 = nnl.nn_softmax_layer()\n",
    "\n",
    "        # cross entropy\n",
    "        self.xent = nnl.nn_cross_entropy_layer()\n",
    "\n",
    "        # initialize momentum parameter\n",
    "        \n",
    "        # learning rate\n",
    "        self.lr = lr\n",
    "        # friction parameter (alpha) for momentum update\n",
    "        self.mmt_friction = mmt_friction\n",
    "\n",
    "    # forward method\n",
    "    # parameters:\n",
    "    #   xx: input MNIST images in batch\n",
    "    #   y: ground truth/labels of the batch\n",
    "    \n",
    "    def forward(self, xx, y):\n",
    "        ########################\n",
    "        # Q1. Complete forward method\n",
    "        ########################\n",
    "        # cv1_f, ac1_f, mp1_f, fc1_f, ac2_f, fc2_f, sm1_f, cn_f\n",
    "        # are outputs from each layer of the CNN\n",
    "        # cv1_f is the output from the convolutional layer\n",
    "        # ac1_f is the output from 1st activation layer\n",
    "        # mp1_f is the output from maxpooling layer\n",
    "        # ... and so on\n",
    "\n",
    "        cv1_f = self.conv_layer_1.forward(xx)\n",
    "\n",
    "        # similarly, fill in ... part in the below\n",
    "        ac1_f = self.act_1.forward(cv1_f)\n",
    "        mp1_f = self.maxpool_layer_1.forward(ac1_f)\n",
    "\n",
    "        fc1_f = self.fc1.forward(mp1_f)\n",
    "        ac2_f = self.act_2.forward(fc1_f)\n",
    "\n",
    "        fc2_f = self.fc2.forward(ac2_f)\n",
    "\n",
    "        sm1_f = self.sm1.forward(fc2_f)\n",
    "\n",
    "        cn_f = self.xent.forward(sm1_f, y)\n",
    "\n",
    "        ########################\n",
    "        # Q1 ends here\n",
    "        ########################\n",
    "\n",
    "        scores = sm1_f\n",
    "        loss = cn_f\n",
    "\n",
    "        return scores, loss\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "    \n",
    "    def step(self):\n",
    "\n",
    "      self.conv_layer_1.step(self.lr, self.mmt_friction)\n",
    "      self.fc1.step(self.lr, self.mmt_friction)\n",
    "      self.fc2.step(self.lr, self.mmt_friction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTClassifier_PT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ## initialize each layer of the entire classifier\n",
    "\n",
    "        ########################\n",
    "        # Q2. Complete constructor using PyTorch modules\n",
    "        ########################\n",
    "\n",
    "        # convolutional layer\n",
    "        # input image size: 28 x 28\n",
    "        # filter size: 3 x 3\n",
    "        # input channel size: 1\n",
    "        # output channel size (number of filters): 28\n",
    "\n",
    "        self.conv_layer_1 = nn.Conv2d(in_channels=1, out_channels=28,\n",
    "                                kernel_size = (3, 3))\n",
    "        \n",
    "        # activation: relu\n",
    "        self.act_1 = nn.ReLU() \n",
    "\n",
    "        # activaition map output: map size 26 x 26, 28 channels\n",
    "\n",
    "        # maxpool\n",
    "        self.maxpool_layer_1 = nn.MaxPool2d(kernel_size=(2,2), stride=2)\n",
    "\n",
    "        # after max pool, map size 13 x 13, 28 channels\n",
    "\n",
    "        # fully connected layer 1\n",
    "        # input: 13 x 13 with 28 channels\n",
    "        # output 128\n",
    "        self.fc1 = nn.Linear(in_features=28*13*13, out_features=128) \n",
    "        self.act_2 = nn.ReLU()\n",
    "\n",
    "        # fully connected layer 1\n",
    "        # input 128\n",
    "        # output 10\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=10)\n",
    "        ########################\n",
    "        # Q2. ends here\n",
    "        ########################\n",
    "\n",
    "    # forward method\n",
    "    # parameters:\n",
    "    #   x: input MNIST images in batch\n",
    "    def forward(self, x):\n",
    "        ########################\n",
    "        # Q3. Complete forward method\n",
    "        ########################\n",
    "        # cv1_f, ac1_f, mp1_f, fc1_f, ac2_f, fc2_f\n",
    "        # are outputs from each layer of the CNN\n",
    "        # cv1_f is the output from the convolutional layer\n",
    "        # ac1_f is the output from 1st activation layer\n",
    "        # mp1_f is the output from maxpooling layer\n",
    "        # ... and so on\n",
    "\n",
    "        cv1_f = self.conv_layer_1(x)\n",
    "        ac1_f = self.act_1(cv1_f)\n",
    "        mp1_f = self.maxpool_layer_1(ac1_f)\n",
    "        \n",
    "        # flatten for input to linear layer\n",
    "        mp1_f = mp1_f.reshape(x.shape[0], 28*13*13)\n",
    "        \n",
    "        fc1_f = self.fc1(mp1_f)\n",
    "        ac2_f = self.act_2(fc1_f)\n",
    "        \n",
    "        out_logit = self.fc2(ac2_f)\n",
    "        ########################\n",
    "        # Q3 ends here\n",
    "        ########################\n",
    "\n",
    "        return out_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (60000, 1, 28, 28)\n",
      "Training labels shape:  (60000,)\n",
      "Test data shape:  (10000, 1, 28, 28)\n",
      "Test labels shape:  (10000,)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'zero_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [12], line 125\u001b[0m\n\u001b[0;32m    122\u001b[0m     scores, loss \u001b[39m=\u001b[39m classifier\u001b[39m.\u001b[39mforward(X, y)\n\u001b[0;32m    124\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m--> 125\u001b[0m     classifier\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m    128\u001b[0m \u001b[39m# for tracking training accuracy\u001b[39;00m\n\u001b[0;32m    129\u001b[0m estim \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mravel(torch\u001b[39m.\u001b[39margmax(scores, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n",
      "Cell \u001b[1;32mIn [10], line 93\u001b[0m, in \u001b[0;36mnn_mnist_classifier.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m---> 93\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv_layer_1\u001b[39m.\u001b[39;49mstep(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlr, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmmt_friction)\n\u001b[0;32m     94\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1\u001b[39m.\u001b[39mstep(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmmt_friction)\n\u001b[0;32m     95\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2\u001b[39m.\u001b[39mstep(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmmt_friction)\n",
      "File \u001b[1;32mc:\\Users\\이승우학부휴학컴퓨터학과\\Desktop\\고려대학교\\3학년\\2학기\\딥러닝\\프로그래밍 과제\\nn_layers_pt.py:260\u001b[0m, in \u001b[0;36mnn_convolutional_layer.step\u001b[1;34m(self, lr, friction)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m lr\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_b\n\u001b[0;32m    259\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mzero_()\n\u001b[1;32m--> 260\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mb\u001b[39m.\u001b[39;49mgrad\u001b[39m.\u001b[39;49mzero_()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'zero_'"
     ]
    }
   ],
   "source": [
    "########################\n",
    "## classification: dataset preparation\n",
    "########################\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  \n",
    "    # set default type to float64\n",
    "    torch.set_default_dtype(torch.float64)\n",
    "    \n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "    X_train=np.expand_dims(X_train,axis=1)\n",
    "    X_test=np.expand_dims(X_test,axis=1)\n",
    "\n",
    "    # As a sanity check, we print out the size of the training and test data.\n",
    "    print('Training data shape: ', X_train.shape)\n",
    "    print('Training labels shape: ', y_train.shape)\n",
    "    print('Test data shape: ', X_test.shape)\n",
    "    print('Test labels shape: ', y_test.shape)\n",
    "\n",
    "    # divide train into training and validation\n",
    "    # set the dataset size\n",
    "    # 50000 training and 10000 validation samples\n",
    "    n_train_sample = 50000\n",
    "    n_val_sample = len(y_train) - n_train_sample\n",
    "\n",
    "    # data preprocessing\n",
    "    # normalize pixel values to (0,1)\n",
    "    X_train = X_train.astype('float64') / 255.0\n",
    "    X_test = X_test.astype('float64') / 255.0\n",
    "    \n",
    "    # split data into training and validation dataset\n",
    "    X_s = np.split(X_train, [n_val_sample, ])\n",
    "    X_val = X_s[0]\n",
    "    X_train = X_s[1]\n",
    "\n",
    "    y_s = np.split(y_train, [n_val_sample, ])\n",
    "    y_val = y_s[0]\n",
    "    y_train = y_s[1]\n",
    "    \n",
    "    # create custom datasets: train, val, test\n",
    "    trn_dataset=[]\n",
    "    for d, l in zip(X_train, y_train):\n",
    "        trn_dataset.append((d,l))\n",
    "    \n",
    "    val_dataset=[]\n",
    "    for d, l in zip(X_val, y_val):\n",
    "        val_dataset.append((d,l))\n",
    "    \n",
    "    test_dataset=[]\n",
    "    for d, l in zip(X_test, y_test):\n",
    "        test_dataset.append((d,l))\n",
    "\n",
    "    ########################\n",
    "    # Q. Set learning rate, batch size and total number of epochs for training\n",
    "    # There are no definitive answers, experiement with several hyperparameters\n",
    "    ########################\n",
    "    lr = 0.02\n",
    "    n_epoch = 2\n",
    "    batch_size = 600\n",
    "    val_batch = 100\n",
    "    test_batch = 100\n",
    "\n",
    "    # set friction (alpha) for momentum\n",
    "    friction = 0.9\n",
    "    \n",
    "    ########################\n",
    "    # Set this True for PyTorch module-based classifier\n",
    "    # Set this False for your custom classifier\n",
    "    PYTORCH_BUILTIN = False\n",
    "\n",
    "    # define classifier\n",
    "    if PYTORCH_BUILTIN:\n",
    "        classifier = MNISTClassifier_PT()\n",
    "        # loss function\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.SGD(classifier.parameters(), lr=lr, momentum=friction)\n",
    "    else:\n",
    "        classifier = nn_mnist_classifier(mmt_friction=friction, lr=lr)\n",
    "\n",
    "    # number of steps per epoch\n",
    "    numsteps = int(n_train_sample / batch_size)\n",
    "    \n",
    "    # dataloaders\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        trn_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=val_batch, shuffle=True)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=test_batch, shuffle=True)\n",
    "\n",
    "    do_validation = True\n",
    "\n",
    "    ########################\n",
    "    # training\n",
    "    ########################\n",
    "\n",
    "    for i in range(n_epoch):\n",
    "        \n",
    "        j = 0\n",
    "        trn_accy = 0\n",
    "        \n",
    "        for trn_data in train_loader:\n",
    "            X, y = trn_data\n",
    "            \n",
    "            X = torch.as_tensor(X)\n",
    "            y = torch.as_tensor(y).long()\n",
    "            \n",
    "            if PYTORCH_BUILTIN:\n",
    "                # perform forward, backprop and weight update\n",
    "                scores = classifier(X)\n",
    "                loss = criterion(scores, y)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            else:\n",
    "                # perform forward, backprop and weight update\n",
    "                scores, loss = classifier.forward(X, y)\n",
    "\n",
    "                loss.backward()\n",
    "                classifier.step()\n",
    "            \n",
    "\n",
    "            # for tracking training accuracy\n",
    "            estim = torch.ravel(torch.argmax(scores, axis=1))\n",
    "            trn_accy += torch.sum((estim == y).long()).item() / batch_size\n",
    "            \n",
    "            j+=1\n",
    "\n",
    "            # every 50 loops, check loss\n",
    "            if (j + 1) % 50 == 0:\n",
    "                print('loop count', j + 1)\n",
    "                print('loss', loss.item())\n",
    "\n",
    "                # every 200 loops, print training accuracy\n",
    "                if (j + 1) % 200 == 0:\n",
    "                    print('training accuracy:', trn_accy / 200 * 100, '%')\n",
    "                    trn_accy = 0\n",
    "\n",
    "                    # evaluate the validation accuarcy\n",
    "                    if do_validation:\n",
    "                        # pick 100 random samples from validation set\n",
    "                        print('performing validation!')\n",
    "                        X,y = next(iter(val_loader)) \n",
    "\n",
    "                        X = torch.as_tensor(X)\n",
    "                        y = torch.as_tensor(y).long()\n",
    "                        \n",
    "                        with torch.no_grad():\n",
    "                            \n",
    "                            if PYTORCH_BUILTIN:\n",
    "                                scores = classifier(X)\n",
    "                            else:                                \n",
    "                                scores, _ = classifier.forward(X, y)\n",
    "\n",
    "                            # for tracking training accuracy\n",
    "                            estim = torch.ravel(torch.argmax(scores, axis=1))\n",
    "\n",
    "                            # compare softmax vs y\n",
    "                            val_accy = torch.sum((estim == y).long()).item()\n",
    "                            print('validation accuracy:', val_accy, '%')\n",
    "\n",
    "    ########################\n",
    "    # testing\n",
    "    ########################\n",
    "    # test_batch: accuracy is measured in this batch size\n",
    "    # test_iter: total number of batch iterations to complete testing over test data\n",
    "    # tot_accy: total accuracy\n",
    "\n",
    "    test_batch = 100\n",
    "    test_iter = int(y_test.shape[0] / test_batch)\n",
    "    tot_accy = 0\n",
    "\n",
    "    for test_data in test_loader:\n",
    "        X, y = test_data\n",
    "\n",
    "        X = torch.as_tensor(X)\n",
    "        y = torch.as_tensor(y).long()\n",
    "\n",
    "        # forward pass!\n",
    "        with torch.no_grad():\n",
    "            if PYTORCH_BUILTIN:\n",
    "                scores = classifier(X)\n",
    "            else:\n",
    "                scores, _ = classifier.forward(X, y)\n",
    "                \n",
    "            estim = torch.ravel(torch.argmax(scores, axis=1))\n",
    "            accy = torch.sum((estim == y).long()).item() / test_batch\n",
    "            tot_accy += accy\n",
    "        print('batch accuracy:', accy)\n",
    "\n",
    "    # print out final accuracy\n",
    "    print('total accuray', tot_accy / test_iter)\n",
    "\n",
    "    # set this to True if we want to plot sample predictions\n",
    "    plot_sample_prediction = True\n",
    "\n",
    "    # test plot randomly picked 10 MNIST numbers\n",
    "    if plot_sample_prediction:\n",
    "        num_plot = 10\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        # get a test sample to plot\n",
    "        X_sample, y_sample = next(iter(test_loader))\n",
    "\n",
    "        for i in range(num_plot):\n",
    "\n",
    "            X = torch.as_tensor(X_sample[i:i+1])\n",
    "            y = torch.as_tensor(y_sample[i]).long()\n",
    "\n",
    "            # get prediction from our classifier\n",
    "            if PYTORCH_BUILTIN:\n",
    "                score = classifier(X)\n",
    "            else:\n",
    "                score, _ = classifier.forward(X, y)\n",
    "            \n",
    "            pred = torch.ravel(torch.argmax(score, axis=1))\n",
    "\n",
    "            # if our prediction is correct, the title will be in black color\n",
    "            # otherwise, for incorrect predictions, the title will be in red\n",
    "            if y == pred:\n",
    "              title_color = 'k'\n",
    "            else:\n",
    "              title_color = 'r'\n",
    "\n",
    "            # plot\n",
    "            img = np.squeeze(X_sample[i])\n",
    "            ax = plt.subplot(1, num_plot, i + 1)\n",
    "            plt.imshow(img, cmap=plt.get_cmap('gray'))\n",
    "\n",
    "            ax.set_title('GT:' + str(y.item()) + '\\n Pred:' + str(int(pred)), color=title_color)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24cbd48528ef30644a4c0b3b48f30a048f2ce882986e8eaa6af7187cf578eebd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
